{
  "MichaelBrown": [
    {
      "question": "Why is 'red spots over body' a strong indicator for diseases like Chicken Pox and Acne in Michael's case?",
      "options": [
          "Because the symptom frequently co-occurs with diseases like Chicken Pox and Acne in the training data, leading the model to link them",
          "Because the system directly assigns any skin disease at random whenever it detects this symptom",
          "Because 'red spots over body' is a unique indicator for Dermatologists, which the system always associates",
          "Because the model was trained only on skin diseases, making it less capable of predicting other conditions"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system say Chicken pox is more likely than Acne or Hepatitis C for Michael?",
      "options": [
        "Because symptoms like rash and red spots match Chicken pox cases in the training data",
        "Because the system saw Chicken pox earlier than the other diseases during training",
        "Because Acne was excluded during preprocessing",
        "Because Chicken pox appears first in a large number of training instances"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system list Acne as the second most likely disease even though its probability is much lower than Chicken pox?",
      "options": [
          "Because it's still the second most likely diagnosis based on Michael's symptoms",
          "Because the output ranking reflects relative order, even if the scores are very different",
          "Because the system includes common conditions to make sure they are considered",
          "Because low-probability diseases occasionally appear if no stronger matches exist"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system still show some chance (a low probability) for Hepatitis C in Michael's results?",
      "options": [
          "Because symptoms like fatigue and lethargy partially match cases labeled as Hepatitis C in training",
          "Because the system assigns small probabilities to diseases with marginal symptom overlap",
          "Because liver-related diseases tend to show up when symptoms are vague",
          "Because the system always spreads some chance across diseases (softmax distributes probability), even for weak matches."
      ],
      "answer": "1"
    },
    {
      "question": "In the system's input, how is the symptom 'fatigue' represented?",
      "options": [
        "1 if fatigue is present, 0 if absent",
        "The overall importance score of fatigue across trees",
        "The probability of fatigue predicted by softmax",
        "A continuous measurement from clinical lab results"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system not list a Cardiologistas one of the recommended specialists even though Michael reported fatigue?",
      "options": [
          "Because the system looks at all symptoms, and fatigue overlaps with many non-cardiac conditions",
          "Because fatigue may be ignored by the LLM explanation module",
          "Because Cardiologists are excluded when 'fatigue' is mild",
          "Because skin symptoms dominate the system results, and 'fatigue' is not relevant"
      ],
      "answer": "1"
    },
    {
      "question": "Why do the symptoms 'red_spots_over_body' and 'skin_rash' increase the probability of Chicken pox in the system's result?",
      "options": [
        "Because they appear in  decision trees that vote for Chicken pox",
        "Because the system applies an extra weight to skin-related symptoms",
        "Because skin indicators are scaled higher than other inputs",
        "Because these symptoms tend to bias predictions toward all skin diseases"
      ],
      "answer": "1"
    },




    {
      "question": "If the LLM explanation claims \"elevated CRP levels indicate a stronger diagnosis,\" what follow-up question should you ask to check if the system actually used CRP in its analysis?",
      "options": [
        "Did the disease model include C-reactive protein (CRP) measurements among its input features?",
        "How many decision trees in the forest calculated CRP thresholds for splitting?",
        "Can you show how the symptom list was constructed without lab values?",
        "What step adjusts the predictions based on lab results?"
      ],
      "answer": "1"
    },
    {
      "question": "What is the primary reason for using a Random Forest classifier as the disease prediction model in this system?",
      "options": [
        "To take advantage of the ensemble method's ability to reduce errors and make more reliable predictions",
        "To handle symptoms in chronological order and prioritize the most recently reported ones",
        "To reduce computation time by limiting the number of decisions compared to other models",
        "To ensure that only a single symptom determines the final disease prediction for easier interpretation"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the Random Forest combine the results of 200 trees instead of just one?",
      "options": [
          "Because it reduces overfitting and produces more stable results",
          "Because it is faster than using fewer trees by parallelizing operations",
          "Because using 100s of trees avoids using Gini impurity, making the overall process simpler",
          "Because a large number of trees generate continuous symptom scores"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system still show a small probability for the second-ranked specialist?",
      "options": [
        "Because the model assigns some chance to every specialist",
        "Because the interface is designed to always display two specialists",
        "Because Logistic Regression resets probabilities below a threshold",
        "Because Random Forest re-votes for specialists after disease prediction"
      ],
      "answer": "1"
    },
    {
      "question": "How can a multiclass classifier (e.g., multinomial logistic regression) be used to recommend a specialist?",
      "options": [
        "By taking the top disease predictions and outputting a probability for specialists",
        "By clustering symptom and matching nearest specialists",
        "By mapping symptom directly to specialist without intermediate disease labels",
        "By using continuous lab measurements to adjust specialist scores during training"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system use a one-hot representation of the top 3 diseases (instead of using the symptoms directly) to recommend a specialist?",
      "options": [
        "Because it ensures that each disease is clearly represented and mapped properly to specialists",
        "Because it reduces the number of symptoms considered and ignores less important ones completely",
        "Because it allows the system to use lab test results alongside symptoms for providng the recommendations",
        "Because it automatically ranks symptoms by severity before choosing a specialist"
      ],
      "answer": "1"
    },




    {
      "question": "Could Michael's red spots be caused by something other than Chicken pox? Which follow-up would would help you explore this possibility?",
      "options": [
        "Could the rash be due to an allergic reaction rather than Chicken pox?",
        "Does the model give equal weight to all skin symptoms, causing rash to trigger Chicken pox predictions?",
        "Could a mistake in the disease labels be inflating the Chicken pox score?",
        "Is the symptom co-occurrence matrix biased toward Chicken pox for any rash-like inputs?"
      ],
      "answer": "1"
    },
    {
      "question": "After Michael's scenario, you notice 'Acne' at 10.5%. What question could help you understand why Acne scored lower than Hepatitis C?",
      "options": [
        "Which decision tree paths favored Hepatitis C over Acne?",
        "Could removing a symptom like fatigue increase Acne's score instead?",
        "Did the system prioritize more severe diseases when symptoms overlapped?",
        "Which lab test values influenced the Acne prediction score?"
      ],
      "answer": "1"
    },
    {
      "question": "Why would removing 'skin_rash' from Michael's symptoms affect the chance that the system recommends a Dermatologist?",
      "options": [
          "Because removing it weakens the system's confidence in skin diseases",
          "Because it causes the softmax to ignore skin-related symptoms",
          "Because removing a key symptom causes significant shifts in symptom distributions that the model cannot properly adjust for",
          "Because missing symptoms can affect decision tree splits that lead to specialist predictions"
      ],
      "answer": "1"
    },
    {
      "question": "Why might adding 'itching' as a symptom make Chicken Pox even more likely?",
      "options": [
          "Because itching often appeared with Chicken pox in training data",
          "Because adding symptoms will always increase all probability scores",
          "Because itching has built-in probability weights",
          "Because adding itching reduces scores of other specialists"
      ],
      "answer": "1"
    },
    {
      "question": "If Michael's actual condition was Lupus but the system only gave it a 5% chance, what does that suggest about the system?",
      "options": [
        "It may be misclassifying Lupus symptoms as resembling Chicken pox",
        "It implies Lupus is grouped under Hepatitis C by the model",
        "It indicates the system focuses too much on common diseases",
        "It shows the specialist module overrides disease scores"
      ],
      "answer": "1"
    },
    {
      "question": "If two patients have identical top-3 disease labels but swapped probabilities (Patient A: [0.50, 0.30, 0.20], Patient B: [0.30, 0.50, 0.20]), how will their specialist recommendations compare?",
      "options": [
        "The system will give the same two specialists in the same order",
        "The system will give the same two specialists, but in reversed order",
        "They may get completely different specialists",
        "Cannot tell without symptom data"
      ],
      "answer": "1"
    },
    {
      "question": "How would removing 'red_spots_over_body' from Michael's symptoms change the disease predictions?",
      "options": [
        "Chicken pox probability would likely decrease due to removal of a key skin-related indicator",
        "All predicted disease probabilities would incorrectly redistribute and add up to more than 100%",
        "The probability for Dermatologist recommendation would remain largely unchanged despite the missing symptom",
        "The system would set a random value for the missing symptom"
      ],
      "answer": "1"
    }
  ],
  "JaneSmith": [
    {
      "question": "Why is 'diarrhoea' a strong indicator for Gastroenteritis in the Random Forest?",
      "options": [
        "Because it frequently co-occurs with Gastroenteritis cases in the training data and influences tree splits",
        "Because it boosts the disease probability through softmax amplification",
        "Because the system is programmed to assign digestive diseases when gastrointestinal symptoms are present",
        "Because diarrhoea overrides other symptoms in the feature vector during preprocessing"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the model give Gastroenterologist such a high confidence score in Jane's case?",
      "options": [
        "Because Gastroenteritis was predicted as the top disease and it's strongly associated with that specialist in training data",
        "Because it is the default recommendation for digestive symptoms",
        "Because vomiting is always mapped to Gastroenterologist",
        "Because softmax boosts the first option"
      ],
      "answer": "1"
    },
    {
      "question": "Why might the model choose Malaria as a lower-ranked prediction despite a weak symptom match?",
      "options": [
        "Because it shares partial overlap (e.g., vomiting) with Jane's input, so it gets a small score from a few trees",
        "Because the specialist prediction may influence disease probabilities during postprocessing",
        "Because Malaria appears frequently in similar clinical cases within the training data",
        "Because symptoms like diarrhoea are loosely associated with Malaria in broader diagnostic patterns"
      ],
      "answer": "1"
    },
    {
      "question": "A user sees Malaria at 10.5% and wonders if it's significant. What does this score indicate?",
      "options": [
        "It suggests a low likelihood but not one that can be entirely ruled out",
        "It indicates that Malaria is the most probable diagnosis based on the symptoms",
        "It reflects that the model likely misinterpreted the symptom inputs",
        "It means the input feature vector may not have been processed correctly"
      ],
      "answer": "1"
    },
    {
      "question": "Which symptom from Jane's input is most likely ignored by the model?",
      "options": [
        "None - all binary symptom features are processed by the model",
        "Nausea - because it's subjective",
        "Abdominal pain - it is prefiltered",
        "Vomiting - it is not encoded numerically"
      ],
      "answer": "1"
    },
    {
      "question": "After Jane sees Chronic cholestasis at 22.0%, which question probes why it wasn't at the topmost?",
      "options": [
        "Which symptom led to Gastroenteritis being favored over Chronic cholestasis?",
        "Why is Chronic cholestasis assigned a fixed upper limit in the model?",
        "Does the model penalize liver diseases like cholestasis by default?",
        "Was cholestasis probability reduced due to overlap with unrelated symptoms?"
      ],
      "answer": "1"
    },
    {
      "question": "Why might 'Malaria' still appear in the predictions despite its low score (10.5%)?",
      "options": [
        "Because softmax assigns non-zero probability to all classes based on the model's output logits",
        "Because it may be manually included as part of a baseline condition set",
        "Because the UI logic ensures a minimum number of conditions are shown and adds labels at random",
        "Because the model resets certain thresholds during impurity-based splits"
      ],
      "answer": "1"
    },




    {
      "question": "If the LLM says \"high blood sugar values suggest systemic inflammation,\" which question would reveal that the Random Forest never ingested blood sugar data?",
      "options": [
        "Were blood sugar measurements actually part of the Random Forest's training features?",
        "What was the model's accuracy on samples with elevated blood sugar values?",
        "Why does vomiting correlate with blood sugar in this explanation?",
        "How many trees in the forest reference blood sugar in their split logic?"
      ],
      "answer": "1"
    },
    {
      "question": "Explain in simple terms why averaging tree outputs reduces overfitting.",
      "options": [
        "It smooths out individual tree errors by combining many independent decisions",
        "It always selects the most frequently predicted class from a single tree",
        "It increases the number of splits per tree to make them more accurate",
        "It avoids softmax by distributing votes across all possible classes"
      ],
      "answer": "1"
    },
    {
      "question": "Why use one-hot encoding of disease labels before the Logistic Regression specialist model instead of reusing raw symptoms?",
      "options": [
        "Because the specialist model is trained only on disease-category features, not symptom vectors",
        "Because raw symptom inputs introduce noise that harms logistic regression accuracy",
        "Because one-hot coding simplifies implementation in the feature pipeline",
        "Because tree-based splitting cannot process continuous symptom inputs"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the model not show just one disease, even when the top prediction is much higher than the rest?",
      "options": [
        "To expose uncertainty and offer users alternative plausible diagnoses alongside the top choice",
        "To maintain alignment with the specialist recommendation module's expected input format",
        "To comply with interface guidelines that display multiple options for informed decision-making",
        "To ensure audit logs capture a spectrum of model outputs for each case"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system use symptoms like nausea and diarrhoea instead of asking for lab values?",
      "options": [
        "Because it was trained only on symptom checklists and not lab data",
        "Because lab values are harder to visualize",
        "Because symptoms don't need softmax normalization",
        "Because nausea is always more accurate"
      ],
      "answer": "1"
    },
    {
      "question": "Why is it helpful for a user to see Chronic cholestasis listed even though it's not the top diagnosis?",
      "options": [
        "Because it reveals alternative diagnostic paths, supporting more informed decision-making",
        "Because the UI requires three diseases to maintain consistent formatting",
        "Because seeing rarer conditions improves the softmax calibration",
        "Because listing extra diagnoses triggers deeper LLM explanations"
      ],
      "answer": "1"
    },



    {
      "question": "Why might a patient's ranking of diseases change if they had reported just two symptoms instead of four?",
      "options": [
        "Because decision tree splits depend on specific symptom combinations, so fewer inputs alter the branching logic",
        "Because reducing the number of symptoms automatically increases the prediction confidence",
        "Because with fewer inputs the model defaults to alphabetical ordering of diseases for prediction",
        "Because softmax normalization treats two-symptom vectors differently than four-symptom ones"
      ],
      "answer": "1"
    },
    {
      "question": "Could Jane's diarrhoea be caused by something other than Gastroenteritis? What would you ask?",
      "options": [
        "Could loose stools indicate IBS or a food intolerance rather than Gastroenteritis?",
        "Could the model's learning rate affect diarrhoea predictions?",
        "Could tree depth settings bias the model toward GI conditions?",
        "Could softmax thresholding suppress alternate GI diagnoses?"
      ],
      "answer": "1"
    },
    {
      "question": "How would removing 'vomiting' from Jane's input likely affect the probabilities?",
      "options": [
        "Gastroenteritis probability would decrease",
        "Malaria probability would increase above 50%",
        "Specialist recommendation would switch to Cardiologist",
        "There would be no noticeable effect on the predictions"
      ],
      "answer": "1"
    },
    {
      "question": "If Gastroenteritis and Heartburn share symptoms, what could make the model prefer one over the other?",
      "options": [
        "Distribution of symptoms and decision path frequency in training data",
        "Random fluctuations in symptom encoding during runtime",
        "Minor shifts in input vector due to data input sequence",
        "Language model token prioritization of similar symptom clusters"
      ],
      "answer": "1"
    },
    {
      "question": "If a confirmed case of IBS scored only 8%, what might that indicate about the model's behavior?",
      "options": [
        "It suggests the model confuses IBS symptoms with Gastroenteritis",
        "It implies IBS is captured under Chronic cholestasis predictions",
        "It indicates the specialist classifier demoted IBS scores",
        "It reflects perfect generalization across GI conditions"
      ],
      "answer": "1"
    },
    {
      "question": "If two patients have different top-3 labels but same probabilities in the same order, how will their specialist recommendations compare?",
      "options": [
        "Same two specialists in same order",
        "Same two specialists in reversed order",
        "Completely different specialists",
        "Cannot tell without symptom data"
      ],
      "answer": "3"
    },
    {
      "question": "What would happen if only two disease labels were passed to the specialist classifier?",
      "options": [
        "It would still generate a probability distribution over only the provided classes with reduced context",
        "The specialist classifier would produce an error because the input vector dimensions are insufficient",
        "It would replicate one of the provided disease labels to populate the missing vector positions",
        "It would return only a single specialist recommendation due to incomplete label inputs"
      ],
      "answer": "1"
    }
  ],
  "AliceJohnson": [
    {
      "question": "Why might the Random Forest associate 'slurred_speech' more with neurological issues than metabolic ones?",
      "options": [
        "Because it is associated with neurological conditions in the training data and is used in tree splits favoring those labels",
        "Because slurred speech is given a higher weight during feature normalization",
        "Because the system uses softmax to increase scores for neurologically relevant symptoms",
        "Because it is hardcoded to trigger Neurologist recommendations"
      ],
      "answer": "1"
    },
    {
      "question": "Why does visual disturbance increase the likelihood of Migraine in the Random Forest model?",
      "options": [
        "Because visual issues frequently appeared in Migraine cases during training",
        "Because visual symptoms activate the model's softmax function",
        "Because dizziness and vision problems are usually grouped together",
        "Because Logistic Regression boosts symptoms related to the brain"
      ],
      "answer": "1"
    },
    {
      "question": "A user sees Hypertension ranked 2nd and wonders if it's less relevant than Migraine. What question helps them understand this?",
      "options": [
        "Which symptoms specifically triggered Hypertension in the random trees used for classification?",
        "What was the overall training accuracy for Hypertension compared to other diseases?",
        "How many features contributed to Hypertension's softmax probability?",
        "Was Hypertension's probability adjusted during normalization of the output?"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the system suggest Neurological conditions for Alice rather than a Cardiac or skin conditions?",
      "options": [
        "Because her symptoms are strongly associated with neurological conditions in the training data",
        "Because the system prefers Neurology when multiple vague symptoms are present",
        "Because Neurological predictions are more likely when headache is included",
        "Because slurred speech does not contribute to cardiac or dermatological diagnoses"
      ],
      "answer": "1"
    },
    {
      "question": "Why might the model assign Hypoglycemia a lower probability even though dizziness is a symptom?",
      "options": [
        "Because dizziness overlaps with Migraine and Hypertension, and the model's trees weigh those conditions more heavily when combined with visual disturbances",
        "Because the feature-engineering pipeline downweights common symptoms like dizziness to reduce noise in decision tree splits",
        "Because the softmax layer inherently distributes probability across all classes rather than excluding low-scoring conditions entirely",
        "Because the symptom preprocessing step may normalize inputs unevenly, reducing the relative influence of dizziness on final scores"
      ],
      "answer": "1"
    },
    {
      "question": "Why might Alice's symptoms be misinterpreted as Hypoglycemia rather than stroke?",
      "options": [
        "Because both conditions share those symptoms and the model saw more Hypoglycemia examples during training",
        "Because the Random Forest arbitrarily ignores neurological features like dizziness",
        "Because the Logistic Regression specialist layer overrides Random Forest outputs",
        "Because the LLM explanation layer biases the diagnosis toward metabolic causes"
      ],
      "answer": "1"
    },
    {
      "question": "Alice is told Neurologist (92%) and Endocrinologist (4%) are top picks. What explains the specialist split?",
      "options": [
        "Because Migraine's high disease probability strongly weights the Neurologist class in the logistic regression",
        "Because the system defaults to Endocrinology as a secondary specialist when probabilities are close",
        "Because class imbalance and regularization push softmax to favor more common specialties",
        "Because the LLM explanation component post-adjusts scores for readability rather than model logic"
      ],
      "answer": "1"
    },




    {
      "question": "When the LLM explanation mentions \"elevated blood pressure indicates neurological risk,\" what would you ask to verify if blood pressure readings were ever used in the model?",
      "options": [
        "Did the model actually use blood pressure measurements as part of its feature inputs?",
        "What Gini impurity threshold triggered splits on blood pressure values?",
        "How many trees predicted Hypoglycemia based on blood pressure?",
        "Why does softmax normalize blood pressure scores in the output?"
      ],
      "answer": "1"
    },
    {
      "question": "Explain why averaging many decision trees reduces model variance.",
      "options": [
        "It smooths out noisy decisions by combining predictions from many diverse trees",
        "It selects the deepest tree to guide the rest during aggregation",
        "It reduces variance by ignoring low-importance features during splits",
        "It switches from Gini to entropy to stabilize predictions"
      ],
      "answer": "1"
    },
    {
      "question": "Why does softmax assign a small probability to the second specialist instead of showing only one?",
      "options": [
        "Because softmax distributes some probability mass to every class based on model logits",
        "Because two specialist outputs are hardcoded into the interface",
        "Because Gini impurity thresholds drive the softmax behavior",
        "Because showing multiple probabilities improves UI clarity"
      ],
      "answer": "1"
    },
    {
      "question": "Although Alice's predicted diseases (Migraine, Hypertension, Hypoglycemia) have similar confidence scores, the system strongly recommends a Neurologist (92.3%). What best explains this?",
      "options": [
        "Because multiple predicted diseases share strong associations with neurological care, boosting their combined specialist score",
        "Because the system defaults to Neurology when disease probabilities are close together",
        "The system averages disease and specialist probabilities directly, so similar inputs yield one high output.",
        "Because softmax forces one specialist to exceed 90% whenever three diseases are predicted"
      ],
      "answer": "1"
    },
    {
      "question": "Why doesn't the model simply return a single most likely diagnosis and specialist?",
      "options": [
        "Because it allows to communicate uncertainty and provide alternative plausible options",
        "Because the softmax layer is implemented to output a full probability distribution by design",
        "Because the API contract requires arrays of predictions for downstream processing",
        "Because specialist recommendations rely on comparative scores across multiple diseases"
      ],
      "answer": "1"
    },
    {
      "question": "Why is it helpful to show confidence scores (e.g. 92% Neurologist, 4% Endocrinologist) instead of only showing labels?",
      "options": [
        "Because it shows how certain the system is and highlights any ambiguity in specialist recommendations",
        "Because the modeL's API always returns numerical outputs alongside labels",
        "Because LLM-derived explanations require quantitative probabilities to generate narratives",
        "Because confidence percentages trigger deeper analytics in downstream modules"
      ],
      "answer": "1"
    },




    {
      "question": "Why might removing 'slurred speech' from Alice's input cause Neurologist to no longer be the top specialist?",
      "options": [
        "Because slurred speech is a critical discriminative feature for neurological conditions, and without it, the probability shifts away from Neurologists",
        "Because the model heavily relies on key neurological symptoms like slurred speech to weight specialist scores, so its absence lowers confidence for all specialists",
        "Because removing slurred speech causes the model to adjust predictions based on remaining symptoms, which the model may not be able to decode",
        "Because missing symptoms cause the input vector to be incomplete and reduce model confidence across all output classes"
      ],
      "answer": "1"
    },
    {
      "question": "Could Alice's headache be caused by dehydration rather than Migraine? What question reveals this?",
      "options": [
        "Could headache indicate dehydration instead of Migraine?",
        "What Logistic Regression solver was used for Migraine prediction?",
        "How many symptoms contributed to the dehydration score?",
        "Why do predicted probabilities for dehydration and Migraine sum to 100%?"
      ],
      "answer": "1"
    },
    {
      "question": "If the true diagnosis were stroke but it only scored 8%, what might that imply about the model?",
      "options": [
        "It may confuse stroke symptoms with those of Migraine",
        "It implies stroke is merged under Hypertension predictions",
        "It indicates the specialist model suppressed stroke scores",
        "It shows the model treats stroke as a rare event"
      ],
      "answer": "1"
    },
    {
      "question": "If two patients have different top-3 labels but the same probabilities in the same order, how will their specialist recommendations compare?",
      "options": [
        "Same specialists same order",
        "Same specialists reversed order",
        "Completely different specialists",
        "Cannot tell without symptom data"
      ],
      "answer": "3"
    },
    {
      "question": "What would happen if only one disease label were passed to the specialist model?",
      "options": [
        "It would produce skewed probability outputs due to lack of sufficient one-hot encoding context",
        "The specialist model would throw an error for missing input dimensions and fail to run",
        "Softmax would collapse to giving full probability to the single class with no alternatives",
        "It would impute zeros for missing classes, leading to distorted specialist probability distribution"
      ],
      "answer": "1"
    },
    {
      "question": "If Alice had only reported dizziness and visual disturbances but not slurred speech or headache, how might the disease and specialist predictions change?",
      "options": [
        "Neurological conditions would remain likely due to the two symptoms being linked to such cases",
        "The model would default to cardiac conditions because dizziness alone favors them strongly",
        "It would trigger an error since at least three symptoms are required for accurate predictions",
        "The prediction would revert to a random distribution because the input vector lacks sufficient features"
      ],
      "answer": "1"
    },
    {
      "question": "If Alice had reported nausea instead of slurred speech, how might the specialist recommendation change?",
      "options": [
        "It might lower the Neurologist's confidence and increase chances of recommending an Endocrinologist",
        "The model would ignore nausea since it is not part of the original input feature set",
        "It would automatically add a Gastroenterologist as the primary specialist because nausea is always linked to that particular specialist",
        "Specialist predictions would remain unchanged because the system only uses the top disease label"
      ],
      "answer": "1"
    }
  ],
  "EmilyDavis": [
    {
      "question": "Why does the model predict Malaria as the top disease for Emily instead of Heart attack or Allergy?",
      "options": [
        "Because multiple symptoms like chills, fever, vomiting, and muscle pain match Malaria patterns learned during training",
        "Because chills are a unique symptom of Malaria and are rarely associated with any other diseases in the dataset",
        "Because softmax tends to emphasize the first disease in the prediction list, which is Malaria in this case",
        "Because the LLM highlights infectious diseases more prominently, influencing model's prediction"
      ],
      "answer": "1"
    },
    {
      "question": "Why might Heart attack appear as the second prediction even though many of Emily's symptoms are non-cardiac?",
      "options": [
        "Because some symptoms like sweating and vomiting overlap between Malaria and cardiac conditions, creating ambiguity in tree paths",
        "Because certain non-cardiac symptoms still contribute partial support to cardiac classes during tree aggregation",
        "Because Heart attack can receive moderate probability if overlapping symptoms trigger multiple branches in the forest",
        "Because ambiguous symptoms can activate decision paths linked to both infectious and cardiac categories"
      ],
      "answer": "1"
    },
    {
      "question": "Emily notices Allergy ranked 3rd and wonders why. What question would clarify that?",
      "options": [
        "Which specific symptoms contributed to Allergy scoring 4%?",
        "Does the model prioritize Allergy when Malaria and Heart attack are uncertain?",
        "Is Allergy added as a fallback when non-skin symptoms are weak?",
        "Was Allergy included because it shares minor symptom overlap with all of Emily's symptoms?"
      ],
      "answer": "1"
    },
    {
      "question": "The Internal Medicine specialist received 72% probability. What does that reflect?",
      "options": [
        "A strong learned association between predicted diseases and this specialty",
        "It is the model's default fallback when specialist probabilities are ambiguous",
        "That Heart Attack probability alone determined the specialist recommendation",
        "That symptom severity was ignored in computing specialist scores"
      ],
      "answer": "1"
    },
    {
      "question": "Why might Internal Medicine be the top recommended specialist for Emily despite her symptoms matching Malaria?",
      "options": [
        "Because Internal Medicine covers systemic infections like Malaria and the model learned this association from data",
        "Because the system orders specialists as per the order in training data and Internal Medicine appears first in the list",
        "Because the LLM explanation component introduces a bias toward recommending generalist specialties by default",
        "Because the specialist model fallback logic prioritizes general medicine in ambiguous cases"
      ],
      "answer": "1"
    },
    {
      "question": "Why would a disease like Dengue (not in top-3) be missed by the model even if Emily's symptoms seem consistent with it?",
      "options": [
      "Because Malaria and other top diseases share stronger symptom-path associations in the training trees, resulting in higher scores",
      "Because Dengue cases were intentionally underrepresented in the training dataset, reducing its learned likelihood",
      "Because the softmax normalization step de-emphasizes lower-frequency tropical illnesses in final probability outputs",
      "Because blood chemistry features like platelet count dominate over symptom-only patterns during tree splits"
      ],
      "answer": "1"
    },
    {
      "question": "Why is it important to ask if bloodwork or lab values were actually used in the prediction?",
      "options": [
        "Because explanation layers may reference lab features even if the model was trained only on symptoms",
        "Because including lab values can introduce multicollinearity that skews tree splits",
        "Because numeric lab inputs require additional preprocessing not covered in the standard pipeline",
        "Because probability normalization handles lab-derived and symptom features differently"
      ],
      "answer": "1"
    },


    {
      "question": "If the LLM explanation claims \"blood work results influence the score,\" which question would clarify whether any lab-based blood tests were included in Emily's prediction model?",
      "options": [
        "Were any laboratory blood test results included among the model's input features?",
        "How many trees in the Random Forest referenced blood work in their splitting logic?",
        "What Logistic Regression solver handles lab-derived variables?",
        "Why do the output probabilities sum to 100% when using blood work?"
      ],
      "answer": "1"
    },
    {
      "question": "Explain why averaging many trees reduces prediction variance.",
      "options": [
        "Combining multiple tree predictions helps cancel out inconsistent errors across individual trees",
        "It picks the tree with the highest accuracy and favors its prediction",
        "Averaging over a large number of trees filters out rare diseases that occur with low probability",
        "It converts binary symptom inputs into lab-based measurements for higher precision"
      ],
      "answer": "1"
    },
    {
      "question": "What is the purpose of using one-hot encoding for the top 3 predicted disease labels as input to the specialist recommendation model?",
      "options": [
        "To ensure each predicted disease is treated as a distinct categorical feature",
        "To reduce the dimensionality of the input data and prevent overfitting",
        "To provide a sparse representation of the disease labels, which can improve model performance",
        "To allow the model to capture non-linear relationships between diseases and specialists"
      ],
      "answer": "1"
    },
    {
      "question": "Why does the model use probabilities rather than simply showing a single disease and specialist recommendation?",
      "options": [
        "To convey uncertainty and allow exploration of alternative diagnoses when symptom overlap is high",
        "To provide probability inputs required by downstream modules for alerting and clinical workflows",
        "To comply with interface requirements for displaying complete prediction distributions transparently",
        "To enable risk stratification and threshold-based prioritization in subsequent decision support tools"
      ],
      "answer": "1"
    },
    {
      "question": "How does the specialist recommendation model (Multinomial Logistic Regression) handle the issue of class imbalance, where some specialists may have fewer training examples than others?",
      "options": [
        "By using class_weight=\"balanced\" to adjust each specialist's weight inversely to its training frequency",
        "By employing a weighted loss that penalizes errors on underrepresented specialist classes more heavily",
        "By oversampling cases for specialists with fewer examples to balance the training set",
        "By ignoring imbalance and relying on default parameter settings for specialist learning"
      ],
      "answer": "1"
    },
    {
      "question": "Suppose you wanted to incorporate additional information about Emily's medical history into the disease prediction model. What would be a suitable approach to integrate this new data?",
      "options": [
        "Include historical health features directly in the Random Forest's input feature set alongside symptom indicators",
        "Train a separate model on medical history data and ensemble its probability outputs with the primary classifier's predictions",
        "Apply transfer learning to fine-tune a pre-trained model that already integrates patient history information",
        "Omit medical history from the inputs to avoid potential noise and focus only on current symptom data"
      ],
      "answer": "1"
    },




    {
      "question": "Could Emily's high fever indicate influenza rather than Malaria? Which follow-up probes this?",
      "options": [
        "Could the model be using fever severity thresholds to differentiate influenza from Malaria?",
        "Could softmax weighting favor certain diseases when input values exceed a threshold?",
        "Could the Random Forest's Gini splits ignore temperature readings if they're out of range?",
        "Could missing chills data cause the model to default to influenza predictions?"
      ],
      "answer": "1"
    },
    {
      "question": "Emily's specialist recommendation changed when 'headache' was added. What does this reveal about the model?",
      "options": [
        "That predictions are sensitive to individual symptom inputs and update the specialist probabilities dynamically",
        "That the model deprioritizes previous symptoms automatically when new ones arrive",
        "That neurologist recommendations override other specialties whenever head-related symptoms are present",
        "That softmax reassigns all probability mass to newly added features exclusively"
      ],
      "answer": "1"
    },
    {
      "question": "Why does removing 'muscle_pain' from Emily's symptoms likely reduce the probability of Malaria?",
      "options": [
        "Because muscle pain is one of several key features learned from Malaria cases and contributes to Malaria-leaning tree splits",
        "Because the model's overall prediction confidence decreases without muscle pain in the symptom set",
        "Because without muscle pain, the model's feature interpretation becomes less certain for infectious diseases",
        "Because the disease classifier requires muscle-related symptoms to generate accurate predictions"
      ],
      "answer": "1"
    },
    {
      "question": "What would happen if Emily had entered fewer than three symptoms?",
      "options": [
        "The model may produce low-confidence predictions",
        "The system might fail due to insufficient symptom input",
        "Specialist recommendations may be unavailable without enough symptoms",
        "The one-hot encoding vector would be shorter and impact prediction"
      ],
      "answer": "1"
    },
    {
      "question": "If Emily had reported chills but no fever, what might the model predict differently?",
      "options": [
        "It might reduce Malaria's probability while increasing likelihood of other infections",
        "It would completely ignore chills because 'chills' is irrelevant without 'fever'",
        "It would automatically recommend Internal Medicince as specialist since there is no fever but there is sweating",
        "The probabilities would remain identical because symptoms are binary and both fever and chills contibute '1' in the encoding."
      ],
      "answer": "1"
    },
    {
      "question": "If Emily actually had dengue but it only scored 5%, what might that suggest about the classifier?",
      "options": [
        "It may confuse dengue symptoms with those of Malaria",
        "It implies dengue is treated like a cardiovascular condition",
        "It indicates the model underrepresents mosquito-borne diseases",
        "It shows the system defaults to more common infections"
      ],
      "answer": "1"
    }, 
    {
      "question": "If two patients have similar symptoms but Emily's case scores Malaria highest and another scores Heart Attack highest, what should you ask to understand the difference?",
      "options": [
        "Which symptom paths the trees split on most strongly for each patient?",
        "How the LLM decides between divergent disease outputs?",
        "What bias term the model applies to adjust final scores?",
        "Why the softmax function weights change across cases?"
      ],
      "answer": "1"
    }
  ]
}
